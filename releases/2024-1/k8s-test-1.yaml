apiVersion: v1
kind: Service
metadata:
  name: k8s-test-1
  labels:
    app: k8s-test-1
    service: k8s-test-1
    tryagain: not-today
spec:
  ports:
    - port: 80
      targetPort: 8080
      name: http
    - port: 5701
      targetPort: 5701
      name: hazelcast
  selector:
    app: k8s-test-1
  type: NodePort
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8s-test-1
  labels:
    app: k8s-test-1
spec:
  selector:
    matchLabels:
      app: k8s-test-1
  template:
    metadata:
      labels:
        app: k8s-test-1
    spec:
      serviceAccountName: pais-service
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: provisioner-type
                    operator: In
                    values:
                      - Karpenter
      # Allows pods for this deployment to run on spot nodes (those have a "spot" taint on them)
      tolerations:
        - key: ecms/spot-instance
          operator: Exists
          effect: NoSchedule
      containers:
        - name: k8s-test-1
          image: 486157446650.dkr.ecr.us-east-1.amazonaws.com/k8s-test-1:1.0.0-SNAPSHOT
          imagePullPolicy: Always
          resources:
            requests:
              cpu: "0.5"
              memory: 200Mi
          ports:
            - containerPort: 8080
          env:
            - name: SERVICE_BASEPATH
              value: "/data"
            - name: SERVER_FORWARD_HEADERS_STRATEGY
              value: "framework"
            - name: SERVICE_CLUSTER_STRATEGY
              value: "kubernetes"
            - name: SERVICE_CLUSTER_KUBERNETES_SERVICENAME
              value: "k8s-test-1"
          volumeMounts:
            - name: persistent-storage
              mountPath: /data
          startupProbe:
            httpGet:
              path: /actuator/health
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 2
            failureThreshold: 30
          readinessProbe:
            httpGet:
              path: /actuator/health
              port: 8080
            periodSeconds: 2
            failureThreshold: 30
      volumes:
        - name: persistent-storage
          persistentVolumeClaim:
            claimName: efs-claim
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: k8s-test-1-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: k8s-test-1
  minReplicas: 1
  maxReplicas: 6
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50
  behavior:
    # Here we define the scale-up and scale-down behavior.
    # For scale up (out), we don't perform scaling more often than once every minute.
    # If we need to scale up, we can add up to 1 pod every 30 seconds as needed, or 50% of the pods over 2 minutes, whichever is more.
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Pods
          value: 1
          periodSeconds: 30
        - type: Percent
          value: 50
          periodSeconds: 120
      selectPolicy: Max
    # For scale down (in), we don't perform scaling more often than once every 10 minutes.
    # If we need to scale down, we can remove either 50% of the pods over 5 minutes, or 20% of the pods over 1 minute, whichever is less.
    scaleDown:
      stabilizationWindowSeconds: 600
      policies:
        - type: Percent
          value: 50
          periodSeconds: 300
        - type: Percent
          value: 20
          periodSeconds: 60
      selectPolicy: Min
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: k8s-test-1-pdb
spec:
  minAvailable: 0
  selector:
    matchLabels:
      app: k8s-test-1